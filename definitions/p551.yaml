id: 551
type: PolicyDefinition
metadata:
    policyName: latentinjection.LatentJailbreakFull
    category: ML/AI Scanning
    stage: source
    description: Implementation of Latent Jailbreak, using latent prompt injection- style distraction to get model to write hateful/harmful text. By using the flexible templating language in this module, we actually get broader coverage that subsumes the analyses in the paper. Neat!
    scheduled_policy: false
    scriptId: 476
    variables: ""
    conditionName: ""
    suggestion: ""
